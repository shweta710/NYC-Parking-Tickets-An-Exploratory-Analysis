{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Parking Tickets: An Exploratory Analysis using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The purpose of this case study is to conduct an exploratory data analysis that will help you understand the data that NYC Police Department has collected for parking tickets. Spark will allow us to analyse the full files at high speeds as opposed to taking a series of random samples that will approximate the population.\n",
    "#### For the scope of this analysis, we will analyse the parking tickets over the year 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Sweta Singh & Sudhanshu Singh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pyspark.sql.SparkSession, The entry point to programming Spark with the Dataset and DataFrame API.\n",
    "#A SparkSession can be used create DataFrame and perform several actions on it.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Group Case Study - NYC Parking\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-30-0-31-25.ec2.internal:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Group Case Study - NYC Parking</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f449bf72400>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Parking Ticket data into a PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons Number: bigint, Plate ID: string, Registration State: string, Issue Date: timestamp, Violation Code: int, Vehicle Body Type: string, Vehicle Make: string, Violation Precinct: int, Issuer Precinct: int, Violation Time: string]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing data from a csv file located in the HDFS into a PySpark dataframe\n",
    "NYCparkingDF = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")\n",
    "# cache dataframe for better performance\n",
    "NYCparkingDF.cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons Number: long (nullable = true)\n",
      " |-- Plate ID: string (nullable = true)\n",
      " |-- Registration State: string (nullable = true)\n",
      " |-- Issue Date: timestamp (nullable = true)\n",
      " |-- Violation Code: integer (nullable = true)\n",
      " |-- Vehicle Body Type: string (nullable = true)\n",
      " |-- Vehicle Make: string (nullable = true)\n",
      " |-- Violation Precinct: integer (nullable = true)\n",
      " |-- Issuer Precinct: integer (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema returns schema in tree format\n",
    "NYCparkingDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|      Summons Number|Plate ID|Registration State|    Violation Code| Vehicle Body Type|      Vehicle Make|Violation Precinct|  Issuer Precinct|   Violation Time|\n",
      "+-------+--------------------+--------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|            10803028|10803028|          10803028|          10803028|          10803028|          10803028|          10803028|         10803028|         10803028|\n",
      "|   mean| 6.817447029065661E9|Infinity|              99.0|34.599430455979565|3.9258887134586864| 6519.974025974026| 45.01216260848347|46.82931211508477|909.2857142857143|\n",
      "| stddev|2.3202339623282285E9|     NaN|               0.0|19.359868716323483|0.5013415469252523|18091.257389147086|40.552560268435805|62.66703577269466|791.8453853409226|\n",
      "|    min|          1002884949|   #1MOM|                99|                 0|                00|             ,FREI|                 0|                0|            .240P|\n",
      "|    max|          8585600044|       ~|                WY|                99|               nan|               nan|               933|              997|              nan|\n",
      "+-------+--------------------+--------+------------------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary statistics\n",
    "NYCparkingDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the no of rows .\n",
    "NYCparkingDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Length of columns .\n",
    "len(NYCparkingDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the first 5 rows of the dataframe\n",
    "NYCparkingDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Duplicates values if exists. \n",
    "NYCparkingDF = NYCparkingDF.dropDuplicates()\n",
    "NYCparkingDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates in this dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find the total number of tickets for the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering data to select only the tickets which belong to 2017\n",
    "##### For the scope of this analysis, we will analyse the parking tickets over the year 2017. So we will filter the dataframe for tickets belonging to only 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Year|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+\n",
      "|    8523552492|  BY917D|                NY|2017-03-07 00:00:00|            38|             4DSD|       NISSA|                52|             52|         1055A|      2017|\n",
      "|    8539069866| HHN2773|                NY|2017-05-30 00:00:00|            21|             SUBN|        FORD|                94|             94|         1206P|      2017|\n",
      "|    8486072323|  XADG70|                NJ|2017-06-19 00:00:00|            14|              VAN|        FORD|                14|             14|         1029A|      2017|\n",
      "|    8566105485| 83469MC|                NY|2017-05-24 00:00:00|            78|              VAN|        FORD|               107|            107|         0938P|      2017|\n",
      "|    8515117083|  UHU79T|                NJ|2017-05-08 00:00:00|            21|             4DSD|       TOYOT|                30|             30|         0915A|      2017|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #we will also register this as a temporary view so that we can query it with SQL and show off basic transformations in SQL\n",
    "NYCparkingDF.createOrReplaceTempView(\"parking_2017\")\n",
    "\n",
    "# Creating a new column called Issue_Yr by taking year part from 'Issue Date'\n",
    "NYCparking2017DF = spark.sql(\"SELECT *, substr(`Issue Date`, 1,4) as Issue_Year FROM parking_2017 where substr(`Issue Date`, 1,4) = '2017'\")\n",
    "NYCparking2017DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|total_tickets_before_filter|\n",
      "+---------------------------+\n",
      "|                   10803028|\n",
      "+---------------------------+\n",
      "\n",
      "+---------------------+\n",
      "|total_tickets_in_2017|\n",
      "+---------------------+\n",
      "|              5431918|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing total no of tickets before and after filtering for Issue Yr = 2017\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import countDistinct\n",
    "NYCparkingDF.select(countDistinct(\"Summons Number\").alias(\"total_tickets_before_filter\")).show()\n",
    "NYCparking2017DF.select(countDistinct(\"Summons Number\").alias(\"total_tickets_in_2017\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total no of tickets in the dataframe = 10803028\n",
    "#### Total no of tickets in the dataframe for the year 2017 = 5431918"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find out the number of unique states from where the cars that got parking tickets came."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out the top 5 States with cars that got highest parking tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n",
      "|Registration State|total_tickets|\n",
      "+------------------+-------------+\n",
      "|                NY|      4273951|\n",
      "|                NJ|       475825|\n",
      "|                PA|       140286|\n",
      "|                CT|        70403|\n",
      "|                FL|        69468|\n",
      "+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating count of tickets after grouping the dataframe by \"Registration State\" and sorting in descending order\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "from pyspark.sql.functions import col, asc, desc\n",
    "NYCparking2017DF.groupBy(\"Registration State\").agg(expr(\"count(`Summons Number`)\")\\\n",
    "                                                .alias(\"total_tickets\")).sort(desc(\"total_tickets\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " New York has got the highest number of cars with parking tickets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding out the distinct values of such States(unique states where the cars getting parking tickets are coming from)\n",
    "#### and total no of States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Registration State|\n",
      "+------------------+\n",
      "|                99|\n",
      "|                AB|\n",
      "|                AK|\n",
      "|                AL|\n",
      "|                AR|\n",
      "|                AZ|\n",
      "|                BC|\n",
      "|                CA|\n",
      "|                CO|\n",
      "|                CT|\n",
      "|                DC|\n",
      "|                DE|\n",
      "|                DP|\n",
      "|                FL|\n",
      "|                FO|\n",
      "|                GA|\n",
      "|                GV|\n",
      "|                HI|\n",
      "|                IA|\n",
      "|                ID|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------------------------+\n",
      "|count(DISTINCT Registration State)|\n",
      "+----------------------------------+\n",
      "|                                65|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct `Registration State` FROM parking_2017\").sort(asc(\"Registration State\")).show()\n",
    "NYCparking2017DF.select(countDistinct(\"Registration State\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that there is a numeric entry '99' in the Registration State column, which should be corrected and as we have found it to be NY, replaced with the state having the maximum entries - in this case which is NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT Registration State)|\n",
      "+----------------------------------+\n",
      "|                                64|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Replacing '99' by 'NY'\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "NYCparking2017DF = NYCparking2017DF.withColumn(\"Registration State\", \\\n",
    "                                       when(NYCparking2017DF[\"Registration State\"] == 99, 'NY').\\\n",
    "                                       otherwise(NYCparking2017DF[\"Registration State\"]))\n",
    "\n",
    "NYCparking2017DF.select(countDistinct(\"Registration State\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After replacing '99' by NY, total no of unique states = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "NYCparking2017DF.createOrReplaceTempView(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropNullDF = NYCparking2017DF.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT Summons Number)|\n",
      "+------------------------------+\n",
      "|                       5431918|\n",
      "+------------------------------+\n",
      "\n",
      "+------------------------------+\n",
      "|count(DISTINCT Summons Number)|\n",
      "+------------------------------+\n",
      "|                       5431918|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import countDistinct\n",
    "NYCparking2017DF.select(countDistinct(\"Summons Number\")).show()\n",
    "dropNullDF.select(countDistinct(\"Summons Number\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How often does each violation code occur? Display the frequency of the top five violation codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping the data based on Violation Code and displaying the top 5 occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|Violation Code|total_violations|\n",
      "+--------------+----------------+\n",
      "|            21|          768087|\n",
      "|            36|          662765|\n",
      "|            38|          542079|\n",
      "|            14|          476664|\n",
      "|            20|          319646|\n",
      "+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "NYCparking2017DF.groupBy(\"Violation Code\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above table shows the total violations against each code, top 5 are present in the answer. Violation Code 21 occured the maximum number of times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'? (Hint: Find the top 5 for both.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping the data based on Vehicle Body Type and Vehicle Make and displaying the top 5 results for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n",
      "|Vehicle Body Type|total_violations|\n",
      "+-----------------+----------------+\n",
      "|             SUBN|         1883954|\n",
      "|             4DSD|         1547312|\n",
      "|              VAN|          724029|\n",
      "|             DELV|          358984|\n",
      "|              SDN|          194197|\n",
      "+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+----------------+\n",
      "|Vehicle Make|total_violations|\n",
      "+------------+----------------+\n",
      "|        FORD|          636844|\n",
      "|       TOYOT|          605291|\n",
      "|       HONDA|          538884|\n",
      "|       NISSA|          462017|\n",
      "|       CHEVR|          356032|\n",
      "+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NYCparking2017DF.groupBy(\"Vehicle Body Type\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(5)\n",
    "                                           \n",
    "NYCparking2017DF.groupBy(\"Vehicle Make\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(5)                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above shows the top results for both Vehicle Body Type and Vehicle Make with total number of violations each made for the top 5 kinds of each.\n",
    "'SUBN' and 'FORD' are responsible for creating maximum violations in Vehicle Body make and vehicle make categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A precinct is a police station that has a certain zone of the city under its command. Find the (5 highest) frequencies of tickets for each of the following: \n",
    "###### 1.'Violation Precinct' (This is the precinct of the zone where the violation occurred). Using this, can you draw any insights for parking violations in any specific areas of the city?\n",
    "###### 2.'Issuer Precinct' (This is the precinct that issued the ticket.)\n",
    "Here, you would have noticed that the dataframe has the'Violating Precinct' or 'Issuing Precinct' as '0'. These are erroneous entries. Hence, you need to provide the records for five correct precincts. (Hint: Print the top six entries after sorting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping the data based on 'Violation Precinct' & 'Issuer Precinct' and displaying the top 6 occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n",
      "|Violation Precinct|total_violations|\n",
      "+------------------+----------------+\n",
      "|                 0|          925596|\n",
      "|                19|          274445|\n",
      "|                14|          203553|\n",
      "|                 1|          174702|\n",
      "|                18|          169131|\n",
      "|               114|          147444|\n",
      "+------------------+----------------+\n",
      "only showing top 6 rows\n",
      "\n",
      "+---------------+----------------+\n",
      "|Issuer Precinct|total_violations|\n",
      "+---------------+----------------+\n",
      "|              0|         1078406|\n",
      "|             19|          266961|\n",
      "|             14|          200495|\n",
      "|              1|          168740|\n",
      "|             18|          162994|\n",
      "|            114|          144054|\n",
      "+---------------+----------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NYCparking2017DF.groupBy(\"Violation Precinct\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(6)\n",
    "                                           \n",
    "NYCparking2017DF.groupBy(\"Issuer Precinct\").agg(expr(\"count(`Summons Number`)\").\\\n",
    "                                           alias(\"total_violations\")).sort(desc(\"total_violations\")).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19th Violation Precinct recoreded the maximum number of violations i.e. 925596 and 19th Precinct itself issued the maximum violations. 0 simply mean that the data wasn't filled so 19th tops as per the true data we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find the violation code frequencies for three precincts that have issued the most number of tickets. Do these precinct zones have an exceptionally high frequency of certain violation codes? Are these codes common across precincts? \n",
    "(Hint: In the SQL view, use the 'where' attribute to filter among three precincts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the three precincts that have issued the most number of tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|Issuer Precinct|total_violation|\n",
      "+---------------+---------------+\n",
      "|             19|         266961|\n",
      "|             14|         200495|\n",
      "|              1|         168740|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT `Issuer Precinct`, COUNT(*) AS total_violation \\\n",
    "FROM parking_2017 \\\n",
    "WHERE `Issuer Precinct` != 0 \\\n",
    "group by `Issuer Precinct` \\\n",
    "order by total_violation desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the 5 most commonly occuring violation codes from the three precincts that have issued the most number of tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+----+\n",
      "|Issuer Precinct|Violation Code|total_violation|RANK|\n",
      "+---------------+--------------+---------------+----+\n",
      "|              1|            14|          38354|   1|\n",
      "|              1|            16|          19081|   2|\n",
      "|              1|            20|          15408|   3|\n",
      "|              1|            46|          12745|   4|\n",
      "|              1|            38|           8535|   5|\n",
      "|             19|            46|          48445|   1|\n",
      "|             19|            38|          36386|   2|\n",
      "|             19|            37|          36056|   3|\n",
      "|             19|            14|          29797|   4|\n",
      "|             19|            21|          28415|   5|\n",
      "|             14|            14|          45036|   1|\n",
      "|             14|            69|          30464|   2|\n",
      "|             14|            31|          22555|   3|\n",
      "|             14|            47|          18364|   4|\n",
      "|             14|            42|          10027|   5|\n",
      "+---------------+--------------+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Issuer Precinct`, `Violation Code`, total_violation, RANK  from \\\n",
    "          (select *, row_number() over (PARTITION by `Issuer Precinct` order by total_violation desc) as RANK from \\\n",
    "            (SELECT b.`Violation Code`, b.`Issuer Precinct`, COUNT(*) AS total_violation \\\n",
    "            FROM (SELECT `Issuer Precinct`, COUNT(*) AS total_violation \\\n",
    "            FROM parking_2017 \\\n",
    "            WHERE `Issuer Precinct` != 0 \\\n",
    "            group by `Issuer Precinct` \\\n",
    "            order by total_violation desc limit 3) as a \\\n",
    "            inner join parking_2017 as b on a.`Issuer Precinct` = b.`Issuer Precinct` \\\n",
    "            group by b.`Violation Code`,b.`Issuer Precinct`) as c) where RANK < 6\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The most commonly occuring violation code for different Precincts are -\n",
    " - Precinct 1 -> 14 (Much higher than the second ranking violation code, almost twice)\n",
    " - Precinct 14 -> 14 \n",
    " - Precinct 19 -> 46 ( Quite higher than the second ranhked violation code)\n",
    " \n",
    "### ---*Note from above that the violation code - 14 seems to be a common among the top 5 most occuring violation codes for the 3 precincts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find out the properties of parking violations across different times of the day:\n",
    " - Find a way to deal with missing values, if any.\n",
    "(Hint: Check for the null values using 'isNull' under the SQL. Also, to remove the null values, check the 'dropna' command in the API documentation.)\n",
    "\n",
    " - The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups.\n",
    "\n",
    " - Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations.\n",
    "(Hint: Use the CASE-WHEN in SQL view to segregate into bins. To find the most commonly occurring violations, you can use an approach similar to the one mentioned in the hint for question 4.)\n",
    "\n",
    " - Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding out the number of occurences of the following in the dataframe -\n",
    "\n",
    " - NULL\n",
    " - BLANK\n",
    " - Nan/nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------------------+\n",
      "|count(DISTINCT Violation Time)|count(Violation Time)|\n",
      "+------------------------------+---------------------+\n",
      "|                             0|                    0|\n",
      "+------------------------------+---------------------+\n",
      "\n",
      "+------------------------------+---------------------+\n",
      "|count(DISTINCT Violation Time)|count(Violation Time)|\n",
      "+------------------------------+---------------------+\n",
      "|                             0|                    0|\n",
      "+------------------------------+---------------------+\n",
      "\n",
      "+------------------------------+---------------------+\n",
      "|count(DISTINCT Violation Time)|count(Violation Time)|\n",
      "+------------------------------+---------------------+\n",
      "|                             1|                   16|\n",
      "+------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropNullDF1 = NYCparking2017DF.filter(col(\"Violation Time\").isNull())\n",
    "dropNullDF2 = NYCparking2017DF.where(\"`Violation Time` == ''\")\n",
    "dropNullDF3 = NYCparking2017DF.where((col(\"Violation Time\") == \"nan\") | (col(\"Violation Time\") == \"Nan\"))\n",
    "dropNullDF1.select(countDistinct(\"Violation Time\"), count(\"Violation Time\")).show()\n",
    "dropNullDF2.select(countDistinct(\"Violation Time\"), count(\"Violation Time\")).show()\n",
    "dropNullDF3.select(countDistinct(\"Violation Time\"), count(\"Violation Time\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 16 records in the dataframe that have the value 'Nan' or 'nan' in the column 'Violation Time'. We will be dropping these records from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------------------+\n",
      "|count(DISTINCT Violation Time)|count(Violation Time)|\n",
      "+------------------------------+---------------------+\n",
      "|                          1625|              5431902|\n",
      "+------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NYCparking2017DF = NYCparking2017DF.filter((col(\"Violation Time\") != \"nan\") & (col(\"Violation Time\") != \"Nan\"))\n",
    "\n",
    "NYCparking2017DF.select(countDistinct(\"Violation Time\"), count(\"Violation Time\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After dropping 'Nan/nan' records from the dataframe, there are 5431902 records left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Violation Time field is specified in a strange format (Eg. 1120A). We will be  transforming it as per the below logic in a new column Violation Time Fixed:\n",
    "\n",
    " - If the `Violation Time` value ends with 'A', then we will ignore the alphabet 'A'. Eg. 1120A -> 1120 as A here is AM.\n",
    " \n",
    " - If the `Violation Time` value ends with 'P', then there are 2 scenarios: \n",
    " \n",
    "  1. If he first 2 digits are '12' then we concatenate '00' with the 3rd and 4th digit and ignore the alphabet 'P'. Eg. 1220P ->      0020.\n",
    "  2. For all other types of occurences, ie. if the `Violation Time` value ends with 'P', and the first 2 digits aren't '12' then      we add '12' to the first 2 digits and concatenate it with the 3rd and 4th digit and ignore the alphabet 'P'. Eg. 0852P ->        2052."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Year|Violation Time Fixed|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+\n",
      "|    8523552492|  BY917D|                NY|2017-03-07 00:00:00|            38|             4DSD|       NISSA|                52|             52|         1055A|      2017|                1055|\n",
      "|    8539069866| HHN2773|                NY|2017-05-30 00:00:00|            21|             SUBN|        FORD|                94|             94|         1206P|      2017|                0006|\n",
      "|    8486072323|  XADG70|                NJ|2017-06-19 00:00:00|            14|              VAN|        FORD|                14|             14|         1029A|      2017|                1029|\n",
      "|    8566105485| 83469MC|                NY|2017-05-24 00:00:00|            78|              VAN|        FORD|               107|            107|         0938P|      2017|                2138|\n",
      "|    8515117083|  UHU79T|                NJ|2017-05-08 00:00:00|            21|             4DSD|       TOYOT|                30|             30|         0915A|      2017|                0915|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NYCparking2017DF = spark.sql(\"SELECT *, \\\n",
    "                        CASE WHEN substr(`Violation Time`, -1,1) == 'A' \\\n",
    "                            THEN substr(`Violation Time`, 1,4) \\\n",
    "                            WHEN substr(`Violation Time`, -1,1) == 'P' AND substr(`Violation Time`, 1,2) == '12'\\\n",
    "                            THEN CONCAT('00',substr(`Violation Time`, 3,2)) \\\n",
    "                            ELSE CONCAT(CAST((substr(`Violation Time`, 1,2) + 12) as int),substr(`Violation Time`, 3,2)) \\\n",
    "                            END AS `Violation Time Fixed` \\\n",
    "FROM parking_2017\")\n",
    "NYCparking2017DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "NYCparking2017DF.createOrReplaceTempView(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dividing the 24 hours into six equal discrete bins of time based on the new column Violation Time Fixed:\n",
    " - Between 0 and 400 --> Dawn\n",
    " - Between 400 and 800 --> Early Morning\n",
    " - Between 800 and 1200 --> Morning \n",
    " - Between 1200 and 1600 --> Afternoon\n",
    " - Between 1600 and 2000 --> Evening\n",
    " - Between 2000 and 2400 --> Night\n",
    " - For everything else --> unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+-------------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Year|Violation Time Fixed|Violation Time Span|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+-------------------+\n",
      "|    4635192880| GGB4757|                NY|2017-06-16 00:00:00|            36|             4DSD|       TOYOT|                 0|              0|         1017A|      2017|                1017|            Morning|\n",
      "|    4635220096| AHV5126|                NY|2017-06-16 00:00:00|            36|             SUBN|         KIA|                 0|              0|         0251P|      2017|                1451|          Afternoon|\n",
      "|    8547250270|  X88HRL|                NJ|2017-04-10 00:00:00|            48|              VAN|       CHEVR|                78|             78|         0950A|      2017|                0950|            Morning|\n",
      "|    8546800741| GEP8446|                NY|2017-04-08 00:00:00|            71|             4DSD|       DODGE|                72|             72|         0157P|      2017|                1357|          Afternoon|\n",
      "|    8489121436| FZV3155|                NY|2017-03-20 00:00:00|            14|             4DSD|       MAZDA|               112|            112|         0109P|      2017|                1309|          Afternoon|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NYCparking2017DF = spark.sql(\"SELECT *, \\\n",
    "                        CASE WHEN `Violation Time Fixed` >= 0 and `Violation Time Fixed` < 400\\\n",
    "                            THEN 'Dawn' \\\n",
    "                            WHEN `Violation Time Fixed` >= 400 and `Violation Time Fixed` < 800\\\n",
    "                            THEN 'Early Morning' \\\n",
    "                            WHEN `Violation Time Fixed` >= 800 and `Violation Time Fixed` < 1200\\\n",
    "                            THEN 'Morning' \\\n",
    "                            WHEN `Violation Time Fixed` >= 1200 and `Violation Time Fixed` < 1600\\\n",
    "                            THEN 'Afternoon' \\\n",
    "                            WHEN `Violation Time Fixed` >= 1600 and `Violation Time Fixed` < 2000\\\n",
    "                            THEN 'Evening' \\\n",
    "                            WHEN `Violation Time Fixed` >= 2000 and `Violation Time Fixed` <= 2400\\\n",
    "                            THEN 'Night' \\\n",
    "                            ELSE 'unknown' \\\n",
    "                            END AS `Violation Time Span` \\\n",
    "FROM parking_2017\")\n",
    "NYCparking2017DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "NYCparking2017DF.createOrReplaceTempView(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the count of records where Violation Time Span = unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-------------------+\n",
      "|Violation Time|Violation Time Fixed|Violation Time Span|\n",
      "+--------------+--------------------+-------------------+\n",
      "|         2203P|                3403|            unknown|\n",
      "|         5028P|                6228|            unknown|\n",
      "|         115+A|                115+|            unknown|\n",
      "|           nan|                null|            unknown|\n",
      "|         5620P|                6820|            unknown|\n",
      "+--------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      89|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Violation Time`,`Violation Time Fixed`, `Violation Time Span`  \\\n",
    "from parking_2017 where `Violation Time Span` == 'unknown'\").show(5)\n",
    "\n",
    "spark.sql(\"select count(*) from parking_2017 where `Violation Time Span` == 'unknown'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are 89 records where Violation Time Span = unknown. These are essentially faulty records and need to be removed. For eg, Violation Time values for some of these records are `6815P`, `110+A`, `093+A`, etc. Thus these records could not be binned into Violation Time Span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+-------------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Year|Violation Time Fixed|Violation Time Span|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+-------------------+\n",
      "|    8497054040| 31416MG|                NY|2017-03-13 00:00:00|            47|             DELV|       FRUEH|                13|             13|         0148P|      2017|                1348|          Afternoon|\n",
      "|    8510966837| 60978MC|                NY|2017-06-01 00:00:00|            69|              VAN|       CHEVR|                19|             19|         1044A|      2017|                1044|            Morning|\n",
      "|    8522162037| AD53168|                CT|2017-05-24 00:00:00|            40|             4DSD|       CHEVR|                20|             20|         0515P|      2017|                1715|            Evening|\n",
      "|    8511812398| GSC7992|                NY|2017-04-07 00:00:00|            38|             4DSD|       HONDA|                19|             19|         1007A|      2017|                1007|            Morning|\n",
      "|    7079862437| ENL9949|                NY|2017-02-25 00:00:00|            14|             4DSD|       NISSA|               103|            103|         0202A|      2017|                0202|               Dawn|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------------------------+--------------------------+\n",
      "|count(DISTINCT Violation Time Span)|count(Violation Time Span)|\n",
      "+-----------------------------------+--------------------------+\n",
      "|                                  6|                   5431829|\n",
      "+-----------------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NYCparking2017DF = NYCparking2017DF.filter(col(\"`Violation Time Span`\") != \"unknown\")\n",
    "NYCparking2017DF.show(5)\n",
    "NYCparking2017DF.select(countDistinct(\"Violation Time Span\"), count(\"Violation Time Span\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After removal of faulty records, the no of rows in the dataframe  = 5431829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "NYCparking2017DF.createOrReplaceTempView(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the 3 most commonly occuring violation codes for each of the 6 different Violation Time Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+----------------+\n",
      "|Violation Time Span|Violation Code|no_of_violations|\n",
      "+-------------------+--------------+----------------+\n",
      "|            Evening|            38|          102855|\n",
      "|            Evening|            14|           75902|\n",
      "|            Evening|            37|           70345|\n",
      "|               Dawn|            21|          107065|\n",
      "|               Dawn|            36|          101991|\n",
      "|               Dawn|            38|           56204|\n",
      "|            Morning|            21|          598062|\n",
      "|            Morning|            36|          348165|\n",
      "|            Morning|            38|          176570|\n",
      "|      Early Morning|            14|           74113|\n",
      "|      Early Morning|            40|           60652|\n",
      "|      Early Morning|            21|           57894|\n",
      "|          Afternoon|            38|          184829|\n",
      "|          Afternoon|            36|          184293|\n",
      "|          Afternoon|            37|          130692|\n",
      "|              Night|             7|           26293|\n",
      "|              Night|            40|           22336|\n",
      "|              Night|            14|           21045|\n",
      "+-------------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Violation Time Span`, `Violation Code`, no_of_violations from \\\n",
    "         (select *, row_number() over (PARTITION by `Violation Time Span` order by no_of_violations desc) as num from \\\n",
    "         (select `Violation Time Span`, `Violation Code`, count(*) as no_of_violations \\\n",
    "         from parking_2017 group by `Violation Time Span`, `Violation Code`) as a) \\\n",
    "         where num < 4\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying the most common Violation Time Span of the day for the three most commonly occurring violation codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+---------------+\n",
      "|Violation Code|Violation Time Span|total_violation|\n",
      "+--------------+-------------------+---------------+\n",
      "|            38|          Afternoon|         184829|\n",
      "|            21|            Morning|         598062|\n",
      "|            36|            Morning|         348165|\n",
      "+--------------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select`Violation Code`,`Violation Time Span`,total_violation from \\\n",
    "          (select *, row_number() over (PARTITION by `Violation Code` order by total_violation desc) as num from \\\n",
    "            (SELECT b.`Violation Code`, b.`Violation Time Span`, COUNT(*) AS total_violation \\\n",
    "            FROM (SELECT `Violation Code`, COUNT(*) AS total_violation \\\n",
    "            FROM parking_2017 \\\n",
    "            group by `Violation Code` \\\n",
    "            order by total_violation desc limit 3) as a \\\n",
    "            inner join parking_2017 as b on a.`Violation Code` = b.`Violation Code` \\\n",
    "            group by b.`Violation Code`,b.`Violation Time Span`) as c) where num < 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Lets try and find some seasonality in this data:\n",
    "\n",
    " - First, divide the year into a certain number of seasons, and find the frequencies of tickets for each season. (Hint: Use Issue Date to segregate into seasons.)\n",
    "\n",
    " - Then, find the three most common violations for each of these seasons. (Hint: You can use an approach similar to the one mentioned in the hint for question 4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "NYCparking2017DF.createOrReplaceTempView(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dividing the year into following number of seasons based on the column Issue Date into a new column  Issue Season:\n",
    " - Month between 3 and 5 --> Spring\n",
    " - Month between 6 and 8 --> Summer\n",
    " - Month between 9 and 11 --> Fall\n",
    " - Else --> Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+-------------------+------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|Issue_Year|Violation Time Fixed|Violation Time Span|Issue Season|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+-------------------+------------+\n",
      "|    8509712827| HNP9733|                NY|2017-03-19 00:00:00|            53|             SUBN|       TOYOT|                 5|              5|         1109A|      2017|                1109|            Morning|      Spring|\n",
      "|    8550208723| 42181JM|                NY|2017-06-20 00:00:00|            10|              VAN|       FRUEH|                61|             61|         1104A|      2017|                1104|            Morning|      Summer|\n",
      "|    7006744659| 28054MC|                NY|2017-02-03 00:00:00|            14|             TRAC|       FRUEH|               109|            109|         1157A|      2017|                1157|            Morning|      Winter|\n",
      "|    4634345766|  923TIJ|                FL|2017-05-23 00:00:00|            36|               4D|       CHEVR|                 0|              0|         0127P|      2017|                1327|          Afternoon|      Spring|\n",
      "|    7494851008| GLP5367|                NY|2017-06-01 00:00:00|            14|             SUBN|       LEXUS|               109|            109|         0431P|      2017|                1631|            Evening|      Summer|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+----------+--------------------+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NYCparking2017DF = spark.sql(\"SELECT *, \\\n",
    "CASE WHEN substr(`Issue Date`,6,2) between 3 and 5 \\\n",
    "    THEN 'Spring' \\\n",
    "    WHEN substr(`Issue Date`,6,2) between 6 and 8 \\\n",
    "    THEN 'Summer' \\\n",
    "    WHEN substr(`Issue Date`,6,2) between 9 and 11 \\\n",
    "    THEN 'Fall' \\\n",
    "    ELSE 'Winter' \\\n",
    "    END AS `Issue Season` \\\n",
    "                        FROM parking_2017\")\n",
    "NYCparking2017DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "NYCparking2017DF.createOrReplaceTempView(\"parking_2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+----------------+\n",
      "|Issue Season|Violation Code|no_of_violations|\n",
      "+------------+--------------+----------------+\n",
      "|      Spring|            21|          402401|\n",
      "|      Spring|            36|          344834|\n",
      "|      Spring|            38|          271167|\n",
      "|      Summer|            21|          127345|\n",
      "|      Summer|            36|           96663|\n",
      "|      Summer|            38|           83518|\n",
      "|        Fall|            46|             231|\n",
      "|        Fall|            21|             128|\n",
      "|        Fall|            40|             116|\n",
      "|      Winter|            21|          238180|\n",
      "|      Winter|            36|          221268|\n",
      "|      Winter|            38|          187385|\n",
      "+------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `Issue Season`, `Violation Code`, no_of_violations from \\\n",
    "         (select *, row_number() over (PARTITION by `Issue Season` order by no_of_violations desc) as num from \\\n",
    "         (select `Issue Season`, `Violation Code`, count(*) as no_of_violations \\\n",
    "         from parking_2017 group by `Issue Season`, `Violation Code`) as a) \\\n",
    "         where num < 4\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Lets take an example of estimating this for the three most commonly occurring codes:\n",
    " - Find the total occurrences of the three most common violation codes.\n",
    " - Then, visit the website: http://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page . It lists the fines associated with different violation codes. Theyre divided into two categories: one for the highest-density locations in the city and the other for the rest of the city. For the sake of simplicity, take the average of the two.\n",
    " - Using this information, find the total amount collected for the three violation codes with the maximum tickets. State the code that has the highest total collection.\n",
    " - What can you intuitively infer from these findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the total occurrences of the three most common violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|Violation Code|no_of_violations|\n",
      "+--------------+----------------+\n",
      "|            21|          768054|\n",
      "|            36|          662765|\n",
      "|            38|          542078|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3violationcodes = spark.sql(\"select `Violation Code`, count(*) as no_of_violations \\\n",
    "         from parking_2017 group by `Violation Code` order by no_of_violations desc limit 3\")\n",
    "top3violationcodes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register DataFrame as temp table\n",
    "top3violationcodes.createOrReplaceTempView(\"violation_codes_df\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing the fines associated with the 3 different violation codes : one for the highest-density locations in the city and the other for the rest of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+---------------------+-----------------+\n",
      "|Violation Code|no_of_violations|High_Density_Loc_Fine|Rest_of_City_Fine|\n",
      "+--------------+----------------+---------------------+-----------------+\n",
      "|            21|          768054|                   65|               45|\n",
      "|            36|          662765|                   50|               50|\n",
      "|            38|          542078|                   65|               35|\n",
      "+--------------+----------------+---------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3violationcodes = spark.sql(\"SELECT *, \\\n",
    "                                CASE WHEN `Violation Code` == 21 \\\n",
    "                                THEN  65 \\\n",
    "                                WHEN `Violation Code` == 36 \\\n",
    "                                THEN 50 \\\n",
    "                                ELSE 65 \\\n",
    "                                END AS `High_Density_Loc_Fine`, \\\n",
    "                                CASE WHEN `Violation Code` == 21 \\\n",
    "                                THEN  45 \\\n",
    "                                WHEN `Violation Code` == 36 \\\n",
    "                                THEN 50 \\\n",
    "                                ELSE 35 \\\n",
    "                                END AS `Rest_of_City_Fine` \\\n",
    "                                FROM violation_codes_df\")\n",
    "top3violationcodes.show(5)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "\n",
    "top3violationcodes.createOrReplaceTempView(\"violation_codes_df\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Taking the average of the 2 categories of fines for each of the 3 different violation codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+---------------------+-----------------+------------+\n",
      "|Violation Code|no_of_violations|High_Density_Loc_Fine|Rest_of_City_Fine|Average Fine|\n",
      "+--------------+----------------+---------------------+-----------------+------------+\n",
      "|            21|          768054|                   65|               45|          55|\n",
      "|            36|          662765|                   50|               50|          50|\n",
      "|            38|          542078|                   65|               35|          50|\n",
      "+--------------+----------------+---------------------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3violationcodes = spark.sql(\"SELECT *, \\\n",
    "                                 cast((`High_Density_Loc_Fine` + `Rest_of_City_Fine`)/2 as int) AS `Average Fine` \\\n",
    "                                FROM violation_codes_df\")\n",
    "top3violationcodes.show(5)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the SQL view to record the changes done to the dataframe\n",
    "\n",
    "top3violationcodes.createOrReplaceTempView(\"violation_codes_df\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the total amount collected for the three violation codes with the maximum tickets by multiplying the average fine per violation to the total occurences of each violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+---------------------+-----------------+------------+----------+\n",
      "|Violation Code|no_of_violations|High_Density_Loc_Fine|Rest_of_City_Fine|Average Fine|Total Fine|\n",
      "+--------------+----------------+---------------------+-----------------+------------+----------+\n",
      "|            21|          768054|                   65|               45|          55|  42242970|\n",
      "|            36|          662765|                   50|               50|          50|  33138250|\n",
      "|            38|          542078|                   65|               35|          50|  27103900|\n",
      "+--------------+----------------+---------------------+-----------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT *, `Average Fine` * no_of_violations as `Total Fine` FROM violation_codes_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violation Code - 21 has the highest total collection in terms of fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the SparkContext Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
